{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as web\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&P500 Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SP500(Dataset):\n",
    "    def __init__(self, x_windows, y_windows, start, end):\n",
    "        #self.symbol = symbol\n",
    "        self.x_windows = x_windows\n",
    "        self.y_windows = y_windows\n",
    "        self.start = datetime.datetime(*start)\n",
    "        self.end = datetime.datetime(*end)\n",
    "        \n",
    "        sp500 = web.DataReader('^GSPC', data_source='yahoo', start=self.start, end=self.end)\n",
    "        \n",
    "        # Normalization\n",
    "        df_columns = sp500.columns\n",
    "        scalar = StandardScaler()\n",
    "        scalar.fit(sp500)\n",
    "\n",
    "        df = scalar.transform(sp500)\n",
    "        df = pd.DataFrame(df, columns=df_columns)\n",
    "        df_label = np.array(df.copy())\n",
    "        \n",
    "        # Make None value\n",
    "        ix = [(row, col) for row in range(df.shape[0]) for col in range(df.shape[1])]\n",
    "        for row, col in random.sample(ix, int(round(.3*len(ix)))): # 30% 을 missing value로 만든다.\n",
    "            df.iat[row, col] = None\n",
    "            \n",
    "        T = df.index.tolist()\n",
    "        row = df.shape[0]\n",
    "        col = df.shape[1]\n",
    "\n",
    "        # Make mask Matrix (1==None value, 0==real value)\n",
    "        M = np.ones([row, col])\n",
    "\n",
    "        M = np.array(df.isnull(), dtype=int)\n",
    "        # 우리는 missing을 0, real을 1로 해야되니까 1-M을 한다.\n",
    "        M = 1 - M \n",
    "        \n",
    "        df = np.array(df)\n",
    "        # Make lag matrix\n",
    "        lag_M = np.zeros([row, col])\n",
    "        for i in range(1, row):\n",
    "            for j in range(col):\n",
    "                if M[i-1][j] == 1:\n",
    "                    lag_M[i][j] = T[i] - T[i-1]\n",
    "\n",
    "                elif M[i-1][j] == 0 and i>0:\n",
    "                    lag_M[i][j] = lag_M[i-1][j] + T[i] - T[i-1]\n",
    "        \n",
    "        X, lag_M, df_label = torch.tensor(df), torch.tensor(lag_M), torch.tensor(df_label)\n",
    "        \n",
    "        # Make Slide Window data\n",
    "        X = torch.tensor(np.nan_to_num(X))\n",
    "        df_label = torch.tensor(np.nan_to_num(df_label))\n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        for i in range(len(X)-(self.x_windows + self.y_windows)):\n",
    "            X_list.append(X[i : i+self.x_windows])\n",
    "            y_list.append(df_label[i+self.x_windows : i+self.x_windows+self.y_windows])\n",
    "        self.X = torch.stack(X_list)\n",
    "        self.y = torch.stack(y_list)\n",
    "\n",
    "\n",
    "        lag_list = []\n",
    "        for i in range(len(lag_M)-(self.x_windows + self.y_windows)):\n",
    "            lag_list.append(lag_M[i : i+self.x_windows])\n",
    "        self.lag_M = torch.stack(lag_list)\n",
    "\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index].float(), self.lag_M[index].float(), self.y[index].float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader 만들기\n",
    "\n",
    "- train data: 1999/01/01 ~ 2020/07/31 \n",
    "- validation data: 2020/08/01 ~ 2020/09/15\n",
    "- test data: 2020/09/16 ~ 2020/10/03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:16: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  app.launch_new_instance()\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:16: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  app.launch_new_instance()\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:16: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SP500(x_windows=10, y_windows=2, start=(1999,1,1), end=(2020,7,31))\n",
    "val_dataset = SP500(x_windows=10, y_windows=2, start=(2020,8,1), end=(2020,9,15))\n",
    "test_dataset = SP500(x_windows=10, y_windows=2, start=(2020, 9, 16), end=(2020, 10, 3))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=512, \n",
    "                                           drop_last=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                         batch_size=512, \n",
    "                                         drop_last=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=512, \n",
    "                                         drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decay vector 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDecay(nn.Module):\n",
    "    def __init__(self, input_size, output_size, device):\n",
    "        super(TemporalDecay, self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "        self.build(self.input_size, self.output_size) # nn.Linear()와 동일, 그저 파라미터를 개별적으로 구현하려고\n",
    "        self.device = device\n",
    "        \n",
    "    def build(self, input_size, output_size):\n",
    "        self.W = Parameter(torch.Tensor(self.output_size, self.input_size))\n",
    "        self.b = Parameter(torch.Tensor(self.output_size))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.W.size(0))\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        if self.b is not None:\n",
    "            self.b.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self,lag_matrix):\n",
    "       \n",
    "        beta = F.relu(F.linear(lag_matrix,self.W, self.b))\n",
    "        beta = 1/torch.exp(beta).to(device)\n",
    "        \n",
    "        return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU-I 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, window_size, device):\n",
    "        super(Model, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        # 일반 gru로는 decay vector를 연산할 수 없어서 하나만 뽑아서 decay를 연산하는 작업을 해야함.\n",
    "        self.grui_cell = nn.GRUCell(input_dim, 256) # GRUCell -> 하나의 셀만 만든다. 여기서 나온 hidden state에 decay vector 곱해서 for문을 돌림.\n",
    "        self.time_decay = TemporalDecay(input_size = input_dim, output_size=1, device=device) # decay vector\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear = nn.Linear(256, input_dim)\n",
    "        self.device = device\n",
    "    def forward(self, x, lag_matrix):\n",
    "        result = []\n",
    "        h = torch.zeros(x.size(0), 256).to(device)\n",
    "        #c = torch.zeros(x.size(0), self.hidden_size).cuda()\n",
    "        \n",
    "        for t in range(self.window_size): # window_size = 10 (seq 개수)\n",
    "            f = lag_matrix[:,t,:] # lage_matrix == [batch, seq, feature] -> seq만 돌린다.\n",
    "            \n",
    "            xinput = x[:,t,:] # x == [batch, seq, feature]\n",
    "         \n",
    "            beta = self.time_decay(f)\n",
    "            beta = beta\n",
    "            h = h * beta # 새로운 hidden vector (decay vector가 곱해진 놈)\n",
    "            h = self.grui_cell(xinput, h)\n",
    "            result.append(h)\n",
    "            result_tensor = torch.stack(result, dim=1)\n",
    "        # print(result_tensor.shape)\n",
    "\n",
    "        output = self.linear(result_tensor[:,-1,:])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/100] | Training loss:0.689 | MAE:0.758 | RMSE:0.978\n",
      "Epoch[1/100] | Training loss:0.367 | MAE:1.125 | RMSE:1.249\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6403dc14c024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Model(input_dim=6, window_size=10, device=device).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "best_mae = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_list = []\n",
    "    y_list = []\n",
    "    output_list = []\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        x = batch[0].to(device)\n",
    "        lag_matrix = batch[1].to(device)\n",
    "        label = batch[2][:,-1,:].to(device)\n",
    "\n",
    "        output = model(x, lag_matrix)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    loss_list = sum(loss_list) / len(loss_list)\n",
    "    \n",
    "    val_loss_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x = batch[0].to(device)\n",
    "            lag_matrix = batch[1].to(device)\n",
    "            label = batch[2][:,-1,:]\n",
    "\n",
    "            output = model(x, lag_matrix)\n",
    "\n",
    "            y_list.append(label.cpu().detach().numpy())\n",
    "            output_list.append(output.cpu().detach().numpy())\n",
    "        \n",
    "        y_list = np.squeeze(y_list)\n",
    "        output_list = np.squeeze(output_list)\n",
    "        mae = mean_absolute_error(y_list, output_list)\n",
    "        rmse = mean_squared_error(y_list, output_list)**0.5\n",
    "\n",
    "        print(f'Epoch[{epoch}/{epochs}] | Training loss:{loss_list:.3f} | MAE:{mae:.3f} | RMSE:{rmse:.3f}')\n",
    "        \n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            torch.save(model.state_dict(), 'grui.pth')\n",
    "print(f'Best MAE:{best_mae:.3f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.974 | RMSE:1.028\n"
     ]
    }
   ],
   "source": [
    "model = Model(input_dim=6, window_size=10, device=device).to(device)\n",
    "model.load_state_dict(torch.load('grui.pth'))\n",
    "model.eval()\n",
    "y_list = []\n",
    "output_list = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        x = batch[0].to(device)\n",
    "        lag_matrix = batch[1].to(device)\n",
    "        label = batch[2][:,-1,:]\n",
    "\n",
    "        output = model(x, lag_matrix)\n",
    "\n",
    "        y_list.append(label.cpu().detach().numpy())\n",
    "        output_list.append(output.cpu().detach().numpy())\n",
    "        \n",
    "    y_list = np.squeeze(y_list)\n",
    "    output_list = np.squeeze(output_list)\n",
    "    mae = mean_absolute_error(y_list, output_list)\n",
    "    rmse = mean_squared_error(y_list, output_list)**0.5\n",
    "    r2 = r2_score(y_list, output_list)\n",
    "\n",
    "    print(f'MAE:{mae:.3f} | RMSE:{rmse:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanila GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SP500(Dataset):\n",
    "    def __init__(self, x_windows, y_windows, start, end):\n",
    "        #self.symbol = symbol\n",
    "        self.x_windows = x_windows\n",
    "        self.y_windows = y_windows\n",
    "        self.start = datetime.datetime(*start)\n",
    "        self.end = datetime.datetime(*end)\n",
    "        \n",
    "        sp500 = web.DataReader('^GSPC', data_source='yahoo', start=self.start, end=self.end)\n",
    "        \n",
    "        # Normalization\n",
    "        df_columns = sp500.columns\n",
    "        scalar = StandardScaler()\n",
    "        scalar.fit(sp500)\n",
    "\n",
    "        df = scalar.transform(sp500)\n",
    "        df = pd.DataFrame(df, columns=df_columns)\n",
    "        df_label = np.array(df.copy())\n",
    "        \n",
    "        # Make None value\n",
    "        ix = [(row, col) for row in range(df.shape[0]) for col in range(df.shape[1])]\n",
    "        for row, col in random.sample(ix, int(round(.3*len(ix)))):\n",
    "            df.iat[row, col] = None\n",
    "        df = df.fillna(0)\n",
    "        #df = df.fillna(df.mean())\n",
    "        df = np.array(df)\n",
    "\n",
    "        X, df_label = torch.tensor(df), torch.tensor(df_label)\n",
    "        \n",
    "        # Make Slide Window data\n",
    "        X = torch.tensor(np.nan_to_num(X))\n",
    "        df_label = torch.tensor(np.nan_to_num(df_label))\n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        for i in range(len(X)-(self.x_windows + self.y_windows)):\n",
    "            X_list.append(X[i : i+self.x_windows])\n",
    "            y_list.append(df_label[i+self.x_windows : i+self.x_windows+self.y_windows])\n",
    "        self.X = torch.stack(X_list)\n",
    "        self.y = torch.stack(y_list)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index].float(), self.y[index].float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:16: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  app.launch_new_instance()\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:16: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  app.launch_new_instance()\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:16: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SP500(x_windows=10, y_windows=2, start=(1999,1,1), end=(2020,7,31))\n",
    "val_dataset = SP500(x_windows=10, y_windows=2, start=(2020,8,1), end=(2020,9,15))\n",
    "test_dataset = SP500(x_windows=10, y_windows=2, start=(2020, 9, 16), end=(2020, 10, 3))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=512, \n",
    "                                           drop_last=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                         batch_size=512, \n",
    "                                         drop_last=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=512, \n",
    "                                         drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = 1\n",
    "        self.gru = nn.GRU(6, 256, num_layers=self.num_layers)\n",
    "        self.linear = nn.Linear(256, 6)\n",
    "        self.device = device\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(1), 256).to(device)\n",
    "        \n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        out = self.linear(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GRU(device).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "best_mae = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_list = []\n",
    "    y_list = []\n",
    "    output_list = []\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        x = batch[0].to(device)\n",
    "        label = batch[1][:,-1,:].to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    loss_list = sum(loss_list) / len(loss_list)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x = batch[0].to(device)\n",
    "            label = batch[1][:,-1,:].to(device)\n",
    "            output = model(x)\n",
    "            y_list.append(label.cpu().detach().numpy())\n",
    "            output_list.append(output.cpu().detach().numpy())\n",
    "            \n",
    "        y_list = np.squeeze(y_list)\n",
    "        output_list = np.squeeze(output_list)\n",
    "        mae = mean_absolute_error(y_list, output_list)\n",
    "        rmse = mean_squared_error(y_list, output_list)**0.5\n",
    "\n",
    "        print(f'Epoch[{epoch}/{epochs}] Training loss:{loss_list:.3f} | MAE:{mae:.3f} | RMSE:{rmse:.3f}')\n",
    "        \n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            torch.save(model.state_dict(), 'gru_imp.pth')\n",
    "print(f'Best MAE:{best_mae:.3f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7338416acd02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gru_imp.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('gru_imp.pth'))\n",
    "model.eval()\n",
    "y_list = []\n",
    "output_list = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        x = batch[0].to(device)\n",
    "        label = batch[1][:,-1,:].to(device)\n",
    "        output = model(x)\n",
    "        y_list.append(label.cpu().detach().numpy())\n",
    "        output_list.append(output.cpu().detach().numpy())\n",
    "\n",
    "    y_list = np.squeeze(y_list)\n",
    "    output_list = np.squeeze(output_list)\n",
    "    mae = mean_absolute_error(y_list, output_list)\n",
    "    rmse = mean_squared_error(y_list, output_list)**0.5\n",
    "    r2 = r2_score(y_list, output_list)\n",
    "\n",
    "    print(f'MAE:{mae:.3f} | RMSE:{rmse:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
